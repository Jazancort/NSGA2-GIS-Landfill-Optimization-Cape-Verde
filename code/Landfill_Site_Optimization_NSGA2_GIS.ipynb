{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e09213ac-274a-444b-88e4-b3835dab1821",
   "metadata": {},
   "source": [
    "## GIS-Based Landfill Site Analysis: Criteria Evaluation and Optimization Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc57913-48f5-4d21-88b2-b9094fd254b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, box\n",
    "import rasterio\n",
    "import rasterio.errors\n",
    "from rasterio.warp import transform\n",
    "import pyogrio.errors \n",
    "import osmnx as ox\n",
    "import numpy as np\n",
    "from scipy.ndimage import sobel \n",
    "import time \n",
    "\n",
    "# --- Helper Functions (Optimized for loop) ---\n",
    "\n",
    "def get_nearest_osm_dist(lat, lon, tags, max_dist=30000, point_crs=\"EPSG:4326\", target_calculation_crs=\"EPSG:32626\"):\n",
    "    \"\"\"Queries OSM for the nearest geometry to (lat, lon) matching the provided tags.\"\"\"\n",
    "    try:\n",
    "        pt_geom = Point(lon, lat)\n",
    "        pt_gdf = gpd.GeoDataFrame([{'id': 1, 'geometry': pt_geom}], crs=point_crs)\n",
    "        ox.settings.use_cache = True\n",
    "        ox.settings.log_console = False \n",
    "        gdf_osm = ox.features_from_point(center_point=(lat, lon), tags=tags, dist=max_dist)\n",
    "        if gdf_osm.empty: return float('nan')\n",
    "        pt_proj = pt_gdf.to_crs(target_calculation_crs)\n",
    "        gdf_osm_proj = gdf_osm.to_crs(target_calculation_crs)\n",
    "        if gdf_osm_proj.empty: return float('nan')\n",
    "        distances = gdf_osm_proj.geometry.distance(pt_proj.geometry.iloc[0])\n",
    "        return distances.min() if not distances.empty else float('nan')\n",
    "    except Exception:\n",
    "        return float('nan')\n",
    "\n",
    "def get_raster_value_from_file(lat, lon, raster_path):\n",
    "    \"\"\"Reads a value from a single-band raster (like pre-calculated slope) at the specified point.\"\"\"\n",
    "    try:\n",
    "        with rasterio.open(raster_path) as src:\n",
    "            xs, ys = transform(\"EPSG:4326\", src.crs, [lon], [lat])\n",
    "            x_proj, y_proj = xs[0], ys[0]\n",
    "            if not (src.bounds.left <= x_proj <= src.bounds.right and src.bounds.bottom <= y_proj <= src.bounds.top):\n",
    "                return float('nan')\n",
    "            row, col = src.index(x_proj, y_proj)\n",
    "            if not (0 <= row < src.height and 0 <= col < src.width): return float('nan')\n",
    "            value_array = src.read(1, window=rasterio.windows.Window(col, row, 1, 1))\n",
    "            if value_array is None or value_array.size == 0: return float('nan')\n",
    "            value = float(value_array[0, 0])\n",
    "            nodata = src.nodata\n",
    "            if nodata is not None and abs(value - nodata) < 1e-6: return float('nan')\n",
    "            return value\n",
    "    except Exception:\n",
    "        return float('nan')\n",
    "\n",
    "def get_aspect_at_point(lat, lon, dem_path_with_elevation_band):\n",
    "    \"\"\"\n",
    "    Calculates terrain aspect in degrees for a point (lat, lon)\n",
    "    from band 1 (elevation) of a DEM file.\n",
    "    Returns aspect (0-360 degrees, 0=North, -1 for flat areas) or float('nan').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with rasterio.open(dem_path_with_elevation_band) as src:\n",
    "            raster_crs = src.crs\n",
    "            bounds = src.bounds\n",
    "\n",
    "            xs, ys = transform(\"EPSG:4326\", raster_crs, [lon], [lat])\n",
    "            x_proj, y_proj = xs[0], ys[0]\n",
    "\n",
    "            xmin, ymin, xmax, ymax = bounds\n",
    "            if not (xmin <= x_proj <= xmax and ymin <= y_proj <= ymax):\n",
    "                print(f\"Projected point ({x_proj:.2f}, {y_proj:.2f}) is outside DEM bounds.\")\n",
    "                return float('nan')\n",
    "\n",
    "            dem_array = src.read(1).astype(np.float32)\n",
    "            nodata_value = src.nodata\n",
    "\n",
    "            cell_size_x = src.transform.a\n",
    "            cell_size_y = abs(src.transform.e)\n",
    "\n",
    "            if nodata_value is not None:\n",
    "                dem_proc = np.where(dem_array == nodata_value, np.nan, dem_array)\n",
    "            else:\n",
    "                dem_proc = dem_array\n",
    "\n",
    "            dz_dx = sobel(dem_proc, axis=1, mode='reflect') / (8 * cell_size_x)\n",
    "            dz_dy_raw = sobel(dem_proc, axis=0, mode='reflect') / (8 * cell_size_y)\n",
    "\n",
    "            aspect_rad = np.arctan2(dz_dx, -dz_dy_raw)\n",
    "            aspect_deg_cartesian = np.degrees(aspect_rad)\n",
    "\n",
    "            aspect_final = (90.0 - aspect_deg_cartesian + 360.0) % 360.0\n",
    "\n",
    "            flat_mask = (np.abs(dz_dx) < 1e-9) & (np.abs(dz_dy_raw) < 1e-9)\n",
    "            aspect_final[flat_mask] = -1.0\n",
    "\n",
    "            if nodata_value is not None:\n",
    "                aspect_final = np.where(np.isnan(dem_proc), nodata_value, aspect_final)\n",
    "\n",
    "            row, col = src.index(x_proj, y_proj)\n",
    "            if not (0 <= row < aspect_final.shape[0] and 0 <= col < aspect_final.shape[1]):\n",
    "                print(f\"Pixel indices ({row}, {col}) are outside aspect array bounds.\")\n",
    "                return float('nan')\n",
    "\n",
    "            aspect_at_point = aspect_final[row, col]\n",
    "            if nodata_value is not None and abs(aspect_at_point - nodata_value) < 1e-6:\n",
    "                print(f\"Aspect value at ({lat}, {lon}) is NoData.\")\n",
    "                return float('nan')\n",
    "\n",
    "            if abs(aspect_at_point - (-1.0)) < 1e-6:\n",
    "                print(f\"Area at ({lat}, {lon}) is flat (aspect = -1).\")\n",
    "            else:\n",
    "                print(f\"Terrain aspect at ({lat}, {lon}): {aspect_at_point:.2f} degrees (0=North).\")\n",
    "\n",
    "            return aspect_at_point\n",
    "\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"I/O error reading DEM file '{dem_path_with_elevation_band}': {e}.\")\n",
    "        return float('nan')\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error calculating aspect at ({lat}, {lon}): {e}\")\n",
    "        return float('nan')\n",
    "\n",
    "def get_distance_to_feature(lat, lon, feature_gdf, point_crs=\"EPSG:4326\"):\n",
    "    \"\"\"Calculates distance from a point to a pre-loaded and reprojected GeoDataFrame of features.\"\"\"\n",
    "    try:\n",
    "        if feature_gdf is None or feature_gdf.empty: return float('nan')\n",
    "        pt_geom = Point(lon, lat)\n",
    "        pt_gdf = gpd.GeoDataFrame([{'id': 1, 'geometry': pt_geom}], crs=point_crs).to_crs(feature_gdf.crs)\n",
    "        pt_point = pt_gdf.geometry.iloc[0]\n",
    "        distances = feature_gdf.geometry.distance(pt_point)\n",
    "        return distances.min()\n",
    "    except Exception:\n",
    "        return float('nan')\n",
    "\n",
    "# --- MAIN WORKFLOW START ---\n",
    "\n",
    "# 1. GENERATE CANDIDATE POINTS\n",
    "# ========================================\n",
    "print(\"Step 1: Generating candidate areas and points...\")\n",
    "island_boundary_path = \".././data/fogo_geology/fogo_geology.shp\"\n",
    "grid_spacing_meters = 1000\n",
    "utm_crs = \"EPSG:32626\"\n",
    "candidate_points_latlon = None\n",
    "\n",
    "if not os.path.exists(island_boundary_path):\n",
    "    print(f\"ERROR: Island boundary file '{island_boundary_path}' not found.\")\n",
    "else:\n",
    "    try:\n",
    "        island_boundary = gpd.read_file(island_boundary_path)\n",
    "        island_boundary_utm = island_boundary.to_crs(utm_crs)\n",
    "        dissolved_boundary = island_boundary_utm.dissolve()\n",
    "        xmin, ymin, xmax, ymax = dissolved_boundary.total_bounds\n",
    "        \n",
    "        grid_x = np.arange(np.floor(xmin), np.ceil(xmax), grid_spacing_meters)\n",
    "        grid_y = np.arange(np.floor(ymin), np.ceil(ymax), grid_spacing_meters)\n",
    "        polygons = [box(x, y, x + grid_spacing_meters, y + grid_spacing_meters) for x in grid_x for y in grid_y]\n",
    "        grid_gdf = gpd.GeoDataFrame(geometry=polygons, crs=utm_crs)\n",
    "        \n",
    "        final_grid = gpd.sjoin(grid_gdf, dissolved_boundary, how=\"inner\", predicate=\"intersects\")\n",
    "        final_grid = final_grid.drop_duplicates(subset=['geometry']).drop(columns=['index_right'], errors='ignore')\n",
    "\n",
    "        centroid_points_utm = final_grid.copy()\n",
    "        centroid_points_utm['geometry'] = centroid_points_utm.geometry.centroid\n",
    "        \n",
    "        candidate_points_latlon = centroid_points_utm.to_crs(\"EPSG:4326\")\n",
    "        \n",
    "        print(f\"Total of {len(candidate_points_latlon)} candidate points generated.\")\n",
    "        print(\"The variable 'candidate_points_latlon' is ready for analysis.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during point generation: {e}\")\n",
    "\n",
    "# 2. ANALYZE CRITERIA FOR EACH CANDIDATE POINT\n",
    "# ======================================================\n",
    "if candidate_points_latlon is not None and not candidate_points_latlon.empty:\n",
    "    print(\"\\nStep 2: Starting criteria analysis for each candidate point...\")\n",
    "    \n",
    "    # --- Data file path definitions ---\n",
    "    dem_elevation_utm_path = '.././data/tif/slope_gdal_degrees.tif'\n",
    "    dem_slope_gdal_path = '.././data/tif/slope_gdal_degrees.tif'\n",
    "    geology_shp_path = '.././data/geology_with_permeability/geology_with_permeability.shp'\n",
    "    protected_areas_shp_path = '.././data/protected_area/protected_area.shp'\n",
    "\n",
    "    # --- Pre-loading vector data for optimization ---\n",
    "    print(\"Pre-loading vector files...\")\n",
    "    metric_crs = \"EPSG:32626\"\n",
    "    low_perm_gdf, protected_areas_gdf = None, None\n",
    "    try:\n",
    "        geology_gdf = gpd.read_file(geology_shp_path)\n",
    "        if geology_gdf.crs is None: geology_gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
    "        low_perm_gdf = geology_gdf[geology_gdf['permeability'] == 'Low'].to_crs(metric_crs)\n",
    "        \n",
    "        protected_areas_gdf = gpd.read_file(protected_areas_shp_path)\n",
    "        if protected_areas_gdf.crs is None: protected_areas_gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
    "        protected_areas_gdf = protected_areas_gdf.to_crs(metric_crs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error pre-loading vector files: {e}.\")\n",
    "\n",
    "    # --- Criteria to be analyzed ---\n",
    "    osm_criteria = {\n",
    "        'distance_to_urban_areas': {'tags': {'landuse': ['residential', 'commercial', 'retail', 'industrial']}},\n",
    "        'distance_to_aerodrome':   {'tags': {'aeroway': 'aerodrome'}},\n",
    "        'distance_to_rivers':      {'tags': {'waterway': ['stream', 'river', 'canal']}},\n",
    "        'distance_to_roads':       {'tags': {'highway': ['motorway', 'trunk', 'primary', 'secondary', 'tertiary', 'unclassified', 'residential', 'service', 'road', 'track', 'path']}},\n",
    "        'distance_to_water_infrastructure': {'tags': {'man_made': ['water_well', 'borehole', 'petroleum_well', 'water_tap', 'water_works']}},\n",
    "        'distance_to_cultivated_areas': {'tags': {'landuse': ['farmland', 'farmyard', 'orchard', 'vineyard', 'allotments']}},\n",
    "        'distance_to_coastline':    {'tags': {'natural': 'coastline'}},\n",
    "    }\n",
    "\n",
    "    # --- Analysis Loop ---\n",
    "    real_data = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_points = len(candidate_points_latlon)\n",
    "    for i, (index, point) in enumerate(candidate_points_latlon.iterrows()):\n",
    "        lat = point.geometry.y\n",
    "        lon = point.geometry.x\n",
    "        \n",
    "        print(f\"--- Analyzing Point {i + 1}/{total_points}: Lat={lat:.4f}, Lon={lon:.4f} ---\")\n",
    "        \n",
    "        point_results = {'latitude (degrees)': lat, 'longitude (degrees)': lon}\n",
    "\n",
    "        # --- Data Collection and Processing ---\n",
    "        # Raster\n",
    "        slope = get_raster_value_from_file(lat, lon, dem_slope_gdal_path)\n",
    "        point_results['slope (degrees)'] = round(slope, 4) if pd.notna(slope) else np.nan\n",
    "        \n",
    "        aspect = get_aspect_at_point(lat, lon, dem_elevation_utm_path)\n",
    "        point_results['aspect (degrees)'] = round(aspect, 4) if pd.notna(aspect) else np.nan\n",
    "\n",
    "        # OSM\n",
    "        for criteria_name, criteria_info in osm_criteria.items():\n",
    "            osm_dist = get_nearest_osm_dist(lat, lon, criteria_info['tags'])\n",
    "            point_results[criteria_name + ' (m)'] = round(osm_dist, 4) if pd.notna(osm_dist) else np.nan\n",
    "        \n",
    "        # Distance to Vectors\n",
    "        dist_perm = get_distance_to_feature(lat, lon, low_perm_gdf)\n",
    "        point_results['distance_to_low_permeability_zones (m)'] = round(dist_perm, 4) if pd.notna(dist_perm) else np.nan\n",
    "        \n",
    "        dist_protected = get_distance_to_feature(lat, lon, protected_areas_gdf)\n",
    "        point_results['distance_to_protected_areas (m)'] = round(dist_protected, 4) if pd.notna(dist_protected) else np.nan\n",
    "        \n",
    "        real_data.append(point_results)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    # --- Finalization ---\n",
    "    results_df = pd.DataFrame(real_data)\n",
    "    \n",
    "    print(\"\\n--- Analysis completed for all points. ---\")\n",
    "    print(f\"Total analysis time for {total_points} points: {duration:.2f} seconds.\")\n",
    "    \n",
    "    # --- MODIFICATION: Rename Columns to Include Units ---\n",
    "    print(\"\\nStep 3: Renaming columns in final DataFrame...\")\n",
    "    column_rename_map = {\n",
    "        'latitude (degrees)': 'latitude (degrees)',\n",
    "        'longitude (degrees)': 'longitude (degrees)',\n",
    "        'slope (degrees)': 'slope (degrees)',\n",
    "        'aspect (degrees)': 'aspect (degrees)',\n",
    "        'distance_to_urban_areas (m)': 'distance_to_urban_areas (m)',\n",
    "        'distance_to_aerodrome (m)': 'distance_to_aerodrome (m)',\n",
    "        'distance_to_rivers (m)': 'distance_to_rivers (m)',\n",
    "        'distance_to_roads (m)': 'distance_to_roads (m)',\n",
    "        'distance_to_water_infrastructure (m)': 'distance_to_water_infrastructure (m)',\n",
    "        'distance_to_cultivated_areas (m)': 'distance_to_cultivated_areas (m)',\n",
    "        'distance_to_coastline (m)': 'distance_to_coastline (m)',\n",
    "        'distance_to_low_permeability_zones (m)': 'distance_to_low_permeability_zones (m)',\n",
    "        'distance_to_protected_areas (m)': 'distance_to_protected_areas (m)'\n",
    "    }\n",
    "    results_df.rename(columns=column_rename_map, inplace=True)\n",
    "    \n",
    "    print(\"\\nSample of final DataFrame with renamed columns:\")\n",
    "    print(results_df.head())\n",
    "\n",
    "    output_csv_path = \"criteria_data_for_optimization_524-3.csv\"\n",
    "    results_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nComplete DataFrame saved to: '{output_csv_path}'\")\n",
    "else:\n",
    "    print(\"\\nNo candidate points were generated. Analysis cannot proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c71aa-62fc-4700-9ba6-d7e7a68ee039",
   "metadata": {},
   "source": [
    "---\n",
    "## Criteria Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c891cd-e5bd-46dc-b387-3ba9354241cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def standardize_csv(file_path):\n",
    "    # Read the original CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Identify column types\n",
    "    coord_columns = ['latitude (degrees)', 'longitude (degrees)']\n",
    "    degree_columns = ['slope (degrees)', 'aspect (degrees)']\n",
    "    distance_columns = [col for col in df.columns if '(m)' in col]\n",
    "\n",
    "    # Create standardized DataFrame\n",
    "    formatted_df = df.copy()\n",
    "\n",
    "    # Apply specific formatting\n",
    "    formatted_df[coord_columns] = df[coord_columns].round(4)\n",
    "    formatted_df[degree_columns] = df[degree_columns].round(2)\n",
    "    formatted_df[distance_columns] = df[distance_columns].round(0).astype(int)\n",
    "\n",
    "    # Generate new filename\n",
    "    base, ext = os.path.splitext(file_path)\n",
    "    new_file = f\"{base}_standardized{ext}\"\n",
    "\n",
    "    # Save to CSV\n",
    "    formatted_df.to_csv(new_file, index=False)\n",
    "\n",
    "    print(f\"Standardized file saved as: {new_file}\")\n",
    "\n",
    "# Example usage\n",
    "standardize_csv(\"criteria_data_for_optimization_524-3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab226436-6bd9-4697-82ef-d93eaf98be21",
   "metadata": {},
   "source": [
    "---\n",
    "## Pareto Optimization: Cost vs Environmental Impact vs Safety Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a10e27-7c60-4622-9752-f7c25324c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymoo.core.problem import ElementwiseProblem\n",
    "from pymoo.core.sampling import Sampling\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.visualization.scatter import Scatter\n",
    "import os\n",
    "\n",
    "# --- 1. Load Real Data ---\n",
    "data_path = \"criteria_data_for_optimization_524-3_standardized-organized.csv\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"ERROR: Data file '{data_path}' not found.\")\n",
    "else:\n",
    "    try:\n",
    "        df = pd.read_csv(data_path, delimiter=';')\n",
    "    except Exception:\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "    print(f\"Data loaded successfully. Total of {len(df)} candidate areas for optimization.\")\n",
    "    \n",
    "    # --- Column Name Adjustments ---\n",
    "    # Rename CSV columns to match what the optimization script expects\n",
    "    # The permeability criterion is now distance-based\n",
    "    name_mapping = {\n",
    "        'latitude (degrees)': 'latitude',\n",
    "        'longitude (degrees)': 'longitude',\n",
    "        'slope (degrees)': 'Slope (degrees)',\n",
    "        'aspect (degrees)': 'Aspect (degrees)',\n",
    "        'distance_to_urban_areas (m)': 'Urban Areas (m)',\n",
    "        'distance_to_aerodrome (m)': 'Aerodrome (m)',\n",
    "        'distance_to_rivers (m)': 'Rivers (m)',\n",
    "        'distance_to_roads (m)': 'Road Network (m)',\n",
    "        'distance_to_water_infrastructure (m)': 'Wells/Boreholes (m)',\n",
    "        'distance_to_cultivated_areas (m)': 'Cultivated Areas (m)',\n",
    "        'distance_to_coastline (m)': 'Coastline (m)',\n",
    "        'distance_to_low_permeability_zones (m)': 'Permeability (Distance) (m)', # UPDATED\n",
    "        'distance_to_protected_areas (m)': 'Protected Areas (m)'\n",
    "    }\n",
    "    df.rename(columns=name_mapping, inplace=True)\n",
    "    \n",
    "    # AHP Weights (Andrade and Barbosa, 2015, Table 5)\n",
    "    # Key \"Permeability (mD)\" changed to \"Permeability (Distance) (m)\"\n",
    "    ahp_weights = {\n",
    "        \"Urban Areas (m)\": 0.166, \"Aerodrome (m)\": 0.102, \n",
    "        \"Permeability (Distance) (m)\": 0.152, # UPDATED\n",
    "        \"Protected Areas (m)\": 0.066, \"Coastline (m)\": 0.069, \"Rivers (m)\": 0.091,\n",
    "        \"Slope (degrees)\": 0.113, \"Road Network (m)\": 0.030, \"Aspect (degrees)\": 0.075,\n",
    "        \"Wells/Boreholes (m)\": 0.055, \"Cultivated Areas (m)\": 0.081\n",
    "    }\n",
    "\n",
    "    # Min and Max for normalization - CALCULATED FROM REAL DATA\n",
    "    valid_columns = [col for col in ahp_weights.keys() if col in df.columns]\n",
    "    min_max_real = {col: (df[col].min(), df[col].max()) for col in valid_columns}\n",
    "    print(\"\\nLimits (Min, Max) calculated from real data for normalization:\")\n",
    "    print(min_max_real)\n",
    "\n",
    "    # 2. Define objective functions for each area\n",
    "    def cost_function(row):\n",
    "        total_cost_score = 0\n",
    "        # Permeability (Distance): Greater distance to low permeability areas -> Higher cost (worse)\n",
    "        val_p, min_p, max_p = row[\"Permeability (Distance) (m)\"], min_max_real[\"Permeability (Distance) (m)\"][0], min_max_real[\"Permeability (Distance) (m)\"][1]\n",
    "        norm_p = (val_p - min_p) / (max_p - min_p) if (max_p - min_p) > 0 else 0\n",
    "        total_cost_score += ahp_weights[\"Permeability (Distance) (m)\"] * norm_p\n",
    "        \n",
    "        # Slope: Higher slope -> Higher cost (worse)\n",
    "        val_d, min_d, max_d = row[\"Slope (degrees)\"], min_max_real[\"Slope (degrees)\"][0], min_max_real[\"Slope (degrees)\"][1]\n",
    "        norm_d = (val_d - min_d) / (max_d - min_d) if (max_d - min_d) > 0 else 0\n",
    "        total_cost_score += ahp_weights[\"Slope (degrees)\"] * norm_d\n",
    "        \n",
    "        # Road Network: Greater distance -> Higher cost (worse)\n",
    "        val_rv, min_rv, max_rv = row[\"Road Network (m)\"], min_max_real[\"Road Network (m)\"][0], min_max_real[\"Road Network (m)\"][1]\n",
    "        norm_rv = (val_rv - min_rv) / (max_rv - min_rv) if (max_rv - min_rv) > 0 else 0\n",
    "        total_cost_score += ahp_weights[\"Road Network (m)\"] * norm_rv\n",
    "        \n",
    "        return total_cost_score\n",
    "\n",
    "    def environmental_impact_function(row):\n",
    "        total_impact_score = 0\n",
    "        # Distance Criteria: Smaller distance -> Higher impact (worse)\n",
    "        for criterion in [\"Protected Areas (m)\", \"Coastline (m)\", \"Rivers (m)\", \"Wells/Boreholes (m)\"]:\n",
    "            val, min_val, max_val = row[criterion], min_max_real[criterion][0], min_max_real[criterion][1]\n",
    "            norm_val = (max_val - val) / (max_val - min_val) if (max_val - min_val) > 0 else 0\n",
    "            total_impact_score += ahp_weights[criterion] * norm_val\n",
    "        \n",
    "        # Aspect: Outside 90-180 range is worse\n",
    "        val_ev = row[\"Aspect (degrees)\"]\n",
    "        min_ev, max_ev = 90, 180 # Favorable range\n",
    "        norm_ev = 0 if (val_ev >= min_ev and val_ev <= max_ev) else 1\n",
    "        total_impact_score += ahp_weights[\"Aspect (degrees)\"] * norm_ev\n",
    "        \n",
    "        return total_impact_score\n",
    "\n",
    "    def safety_function(row):\n",
    "        total_safety_score = 0\n",
    "        # Distance Criteria: Greater distance -> Higher safety (better)\n",
    "        for criterion in [\"Urban Areas (m)\", \"Aerodrome (m)\", \"Cultivated Areas (m)\"]:\n",
    "             val, min_val, max_val = row[criterion], min_max_real[criterion][0], min_max_real[criterion][1]\n",
    "             norm_val = (val - min_val) / (max_val - min_val) if (max_val - min_val) > 0 else 0\n",
    "             total_safety_score += ahp_weights[criterion] * norm_val\n",
    "        return total_safety_score\n",
    "\n",
    "    df[\"CostDef\"] = df.apply(cost_function, axis=1)\n",
    "    df[\"ImpactDef\"] = df.apply(environmental_impact_function, axis=1)\n",
    "    df[\"SafetyDef\"] = df.apply(safety_function, axis=1)\n",
    "\n",
    "    class MySampling(Sampling):\n",
    "        def _do(self, problem, n_samples, **kwargs):\n",
    "            X = np.full((n_samples, problem.n_var), False, dtype=bool)\n",
    "            for i in range(n_samples):\n",
    "                idx = np.random.choice(problem.n_var, problem.n_select, replace=False)\n",
    "                X[i, idx] = True\n",
    "            return X\n",
    "\n",
    "    class SelectAreas(ElementwiseProblem):\n",
    "        def __init__(self, df, n_select=3):\n",
    "            super().__init__(n_var=len(df), n_obj=3, xl=0, xu=1, type_var=bool)\n",
    "            self.df = df\n",
    "            self.n_select = n_select\n",
    "        def _evaluate(self, x, out, *args, **kwargs):\n",
    "            if x.sum() != self.n_select:\n",
    "                out[\"F\"] = [1e10, 1e10, 1e10]\n",
    "                return\n",
    "            idx = np.where(x)[0]\n",
    "            cost = self.df.iloc[idx][\"CostDef\"].sum()\n",
    "            impact = self.df.iloc[idx][\"ImpactDef\"].sum()\n",
    "            positive_safety = self.df.iloc[idx][\"SafetyDef\"].sum()\n",
    "            out[\"F\"] = [cost, impact, -positive_safety]\n",
    "\n",
    "    problem = SelectAreas(df, n_select=3)\n",
    "    algorithm = NSGA2(pop_size=100, sampling=MySampling())\n",
    "    res = minimize(problem, algorithm, ('n_gen', 100), seed=1, verbose=True)\n",
    "\n",
    "    # 4. Show best solutions (Pareto frontier)\n",
    "    if res.F is not None and len(res.F) > 0:\n",
    "        pareto = res.F\n",
    "        valid_solutions_mask = (pareto[:, 0] < 1e9)\n",
    "        pareto_valid = pareto[valid_solutions_mask]\n",
    "        if len(pareto_valid) > 0:\n",
    "            print(\"\\nOptimal solutions (Cost, Impact, -Safety):\")\n",
    "            print(pareto_valid)\n",
    "            plot = Scatter(title=\"Pareto Frontier\", labels=[\"Cost\", \"Impact\", \"-Safety\"])\n",
    "            plot.add(pareto_valid)\n",
    "            plot.show()\n",
    "\n",
    "            if res.X is not None:\n",
    "                X_valid = res.X[valid_solutions_mask]\n",
    "                if len(X_valid) > 0:\n",
    "                    # Lowest cost solution\n",
    "                    idx_lowest_cost = np.argmin(pareto_valid[:,0])\n",
    "                    selected_cost = np.where(X_valid[idx_lowest_cost])[0]\n",
    "                    print(\"\\nAreas selected in LOWEST COST solution:\")\n",
    "                    print(df.iloc[selected_cost])\n",
    "                    \n",
    "                    # Lowest environmental impact solution\n",
    "                    idx_lowest_impact = np.argmin(pareto_valid[:,1])\n",
    "                    selected_impact = np.where(X_valid[idx_lowest_impact])[0]\n",
    "                    print(\"\\nAreas selected in LOWEST ENVIRONMENTAL IMPACT solution:\")\n",
    "                    print(df.iloc[selected_impact])\n",
    "                    \n",
    "                    # HIGHEST SAFETY solution (lowest value of -Safety)\n",
    "                    idx_highest_safety = np.argmin(pareto_valid[:,2])\n",
    "                    selected_safety = np.where(X_valid[idx_highest_safety])[0]\n",
    "                    print(\"\\nAreas selected in HIGHEST SAFETY solution:\")\n",
    "                    print(df.iloc[selected_safety])\n",
    "    else:\n",
    "        print(\"\\nNo solutions found by the algorithm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa3ae9-3003-453d-95ed-93d5256610f5",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi-Objective Optimization Results: Visualizing the Pareto Frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cacc2e7-8091-412e-a24f-b2d876aa8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Extract valid Pareto solutions\n",
    "pareto = res.F\n",
    "valid_solutions_mask = (pareto[:, 0] < 1e9)\n",
    "pareto_valid = pareto[valid_solutions_mask]\n",
    "n_solutions = len(pareto_valid)\n",
    "\n",
    "if n_solutions > 0:\n",
    "    # Set consistent styling\n",
    "    plt.style.use('seaborn-v0_8-bright')\n",
    "    \n",
    "    # 1. 3D Pareto Front Visualization\n",
    "    fig_3d = plt.figure(figsize=(12, 9))\n",
    "    ax_3d = fig_3d.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Create color gradient based on solution index\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, n_solutions))\n",
    "    \n",
    "    scatter_3d = ax_3d.scatter(\n",
    "        pareto_valid[:, 0],  # Cost\n",
    "        pareto_valid[:, 1],  # Environmental Impact\n",
    "        pareto_valid[:, 2],  # -Safety\n",
    "        c=colors, s=60, alpha=0.8, depthshade=True\n",
    "    )\n",
    "    \n",
    "    # Labels and title\n",
    "    ax_3d.set_xlabel('\\nCost (minimize)', fontsize=12, linespacing=2)\n",
    "    ax_3d.set_ylabel('\\nEnvironmental Impact (minimize)', fontsize=12, linespacing=2)\n",
    "    ax_3d.set_zlabel('\\n-Safety (maximize)', fontsize=12, linespacing=2)\n",
    "    ax_3d.set_title(\n",
    "        f'Pareto Front: Trade-off distribution among Cost, Impact, and Safety objectives\\nacross {n_solutions} non-dominated solutions',\n",
    "        fontsize=14, pad=20\n",
    "    )\n",
    "    \n",
    "    # Grid and viewing angle\n",
    "    ax_3d.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax_3d.view_init(elev=25, azim=45)\n",
    "    \n",
    "    # 2. 2D Pairwise Visualizations\n",
    "    fig_2d, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    fig_2d.suptitle(\n",
    "        f'Pairwise Trade-off Analysis of {n_solutions} Pareto-optimal Solutions',\n",
    "        fontsize=16, y=1.02\n",
    "    )\n",
    "    \n",
    "    # Cost vs Environmental Impact\n",
    "    ax1.scatter(pareto_valid[:, 0], pareto_valid[:, 1], c=colors, alpha=0.8)\n",
    "    ax1.set_xlabel('Cost (minimize)', fontsize=12)\n",
    "    ax1.set_ylabel('Environmental Impact (minimize)', fontsize=12)\n",
    "    ax1.set_title('Cost vs Environmental Impact', fontsize=13)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "    \n",
    "    # Cost vs Safety\n",
    "    ax2.scatter(pareto_valid[:, 0], pareto_valid[:, 2], c=colors, alpha=0.8)\n",
    "    ax2.set_xlabel('Cost (minimize)', fontsize=12)\n",
    "    ax2.set_ylabel('-Safety (maximize)', fontsize=12)\n",
    "    ax2.set_title('Cost vs Safety', fontsize=13)\n",
    "    ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "    \n",
    "    # Environmental Impact vs Safety\n",
    "    ax3.scatter(pareto_valid[:, 1], pareto_valid[:, 2], c=colors, alpha=0.8)\n",
    "    ax3.set_xlabel('Environmental Impact (minimize)', fontsize=12)\n",
    "    ax3.set_ylabel('-Safety (maximize)', fontsize=12)\n",
    "    ax3.set_title('Environmental Impact vs Safety', fontsize=13)\n",
    "    ax3.grid(True, linestyle='--', alpha=0.4)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"No valid Pareto solutions to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216e010-fe61-4033-9d78-3b1cf7f2d045",
   "metadata": {},
   "source": [
    "---\n",
    "## Spatial Criteria Interdependencies: Spearman Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc49aa-5f3f-460b-8ac3-0528bd8cd5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# File path\n",
    "file_path = 'criteria_data_for_optimization_524-3_standardized-organized.csv'\n",
    "\n",
    "# EXACT column names in the CSV file, as per your diagnosis\n",
    "actual_column_names = [\n",
    "    'slope (degrees)', 'aspect (degrees)', 'distance_to_urban_areas (m)',\n",
    "    'distance_to_aerodrome (m)', 'distance_to_rivers (m)', 'distance_to_roads (m)',\n",
    "    'distance_to_water_infrastructure (m)', 'distance_to_cultivated_areas (m)', 'distance_to_coastline (m)',\n",
    "    'distance_to_low_permeability_zones (m)', 'distance_to_protected_areas (m)'\n",
    "]\n",
    "\n",
    "# \"Friendly\" names for final presentation in tables and text.\n",
    "# The order here MUST match the order in actual_column_names\n",
    "friendly_column_names = {\n",
    "    'slope (degrees)': 'Slope (degrees)',\n",
    "    'aspect (degrees)': 'Aspect (degrees)',\n",
    "    'distance_to_urban_areas (m)': 'Dist. Urban Areas (m)',\n",
    "    'distance_to_aerodrome (m)': 'Dist. Aerodrome (m)',\n",
    "    'distance_to_rivers (m)': 'Dist. Rivers (m)',\n",
    "    'distance_to_roads (m)': 'Dist. Road Network (m)',\n",
    "    'distance_to_water_infrastructure (m)': 'Dist. Wells/Boreholes (m)',\n",
    "    'distance_to_cultivated_areas (m)': 'Dist. Cultivated Areas (m)',\n",
    "    'distance_to_coastline (m)': 'Dist. Coastline (m)',\n",
    "    'distance_to_low_permeability_zones (m)': 'Dist. Low Permeability Zones (m)',\n",
    "    'distance_to_protected_areas (m)': 'Dist. Protected Areas (m)'\n",
    "}\n",
    "\n",
    "# Load DataFrame with correct delimiter\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Calculate Spearman correlation matrix using ACTUAL column names\n",
    "corr_matrix = df[actual_column_names].corr(method='spearman')\n",
    "\n",
    "# Rename matrix indices and columns to \"friendly\" names for presentation\n",
    "corr_matrix.rename(columns=friendly_column_names, index=friendly_column_names, inplace=True)\n",
    "\n",
    "# Transform matrix into pairs list\n",
    "corr_pairs = corr_matrix.unstack().reset_index()\n",
    "corr_pairs.columns = ['Criterion 1', 'Criterion 2', 'Spearman_rho']\n",
    "corr_pairs = corr_pairs[corr_pairs['Criterion 1'] != corr_pairs['Criterion 2']]\n",
    "corr_pairs['sorted_criteria'] = corr_pairs.apply(lambda r: tuple(sorted((r['Criterion 1'], r['Criterion 2']))), axis=1)\n",
    "corr_pairs = corr_pairs.drop_duplicates(subset='sorted_criteria').drop(columns='sorted_criteria')\n",
    "corr_pairs['abs_rho'] = corr_pairs['Spearman_rho'].abs()\n",
    "strongest_pairs = corr_pairs.sort_values(by='abs_rho', ascending=False).drop(columns='abs_rho')\n",
    "\n",
    "# Final Results\n",
    "print(\">>> CRITERIA PAIRS WITH STRONGEST CORRELATION (Spearman's ρ) <<<\")\n",
    "print(\"Actual values calculated from your data:\")\n",
    "print(strongest_pairs.head(5).round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb62dc3-f499-4471-b266-6df82c4d059b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
